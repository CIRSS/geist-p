#!/usr/bin/python3

import click, jinja2, pickle, json, os, sys, ast, re
from jinja2_simple_tags import StandaloneTag, ContainerTag
from io import StringIO
import pandas as pd
import pygraphviz as pgv
from rdflib import Graph
from owlrl import DeductiveClosure, RDFS_Semantics, OWLRL_Semantics, RDFS_OWLRL_Semantics
from rdflib.plugins.sparql.results.jsonresults import JSONResultSerializer

DATA_DIR = ".geistdata/"
PASTEL_COLORS = ["#b3e2cd", "#fdccac", "#cbd5e8", "#f4cae4", "#e6f5c9", "#fff2ae", "#f1e2cc", "#cccccc"]

@click.group()
def cli():
    pass

def json2df(json_str):
    """
    This function is to convert a JSON string to a Pandas data frame
    :param json_str: a JSON string
    :return df: a Pandas data frame
    """
    return pd.read_json(json_str)

def format_cell(cell):
    """
    This function is to format a cell in a Pandas data frame to a string of N-Triples
    :param cell: a cell in a Pandas data frame
    :return cell: a string of N-Triples
    """
    if cell.startswith("<") or cell.startswith("_:") or cell.startswith('"'):
        return cell
    else:
        return '"' + cell + '"'        

def df2nt(df, colnames_of_triples):
    """
    This function is to convert a Pandas data frame to a string of N-Triples
    :param df: a Pandas data frame
    :param colnames_of_triples: a list of list. [[subject1, predicate1, object1], [subject2, predicate2, object2], ...] where these items are column names
    :return nt: a string of N-Triples
    """
    nt = ""
    for colnames_of_triple in colnames_of_triples:
        nt += df.apply(lambda row: "{s} {p} {o} .\n".format(s=format_cell(row[colnames_of_triple[0]]), p=format_cell(row[colnames_of_triple[1]]), o=format_cell(row[colnames_of_triple[2]])), axis=1).str.cat(sep='')
    return nt

def infer_rdf_graph(rdf_graph, infer):
    if infer == "rdfs":
        # RDFS_Semantics: implementing the RDFS semantics
        DeductiveClosure(RDFS_Semantics).expand(rdf_graph)
    elif infer == "owl":
        # OWLRL_Semantics: implementing the OWL 2 RL
        DeductiveClosure(OWLRL_Semantics).expand(rdf_graph)
    elif infer == "rdfs_owl":
        # RDFS_OWLRL_Semantics: implementing a combined semantics of RDFS semantics and OWL 2 RL
        DeductiveClosure(RDFS_OWLRL_Semantics).expand(rdf_graph)
    return rdf_graph

def create_rdf_graph(input_file, input_format, colnames_of_triples, infer):
    """
    This function is to load a file with a given format as a RDF Graph object supported by RDFLib
    :param input_file: String. Path of the file
    :param input_format: Defaults to json-ld. It must be one of [xml, n3, turtle, nt, pretty-xml, trix, trig, nquads, json-ld, hext, csv]
    :param infer: Inference to perform on update [none, rdfs, owl, rdfs_owl] (default "none")
    :return rdf_graph: a RDF Graph object supported by RDFLib
    """
    if input_format not in ["xml", "n3", "turtle", "nt", "pretty-xml", "trix", "trig", "nquads", "json-ld", "hext", "csv"]:
        raise ValueError("Only supports [xml, n3, turtle, nt, pretty-xml, trix, trig, nquads, json-ld, hext, csv], but '" + str(input_format) + "' was given.")
    if infer not in ["none", "rdfs", "owl", "rdfs_owl"]:
        raise ValueError("Only [none, rdfs, owl, rdfs_owl] are supported.")
    # Parse the file (e.g., a JSON-LD file) with RDFLib
    rdf_graph = Graph()
    if input_format == "csv":
        if not colnames_of_triples:
            raise ValueError("Please provide the column names of triples for the CSV file.")
        nt = df2nt(pd.read_csv(StringIO(input_file)), ast.literal_eval(colnames_of_triples))
        rdf_graph.parse(data=nt, format="nt")
    else:
        rdf_graph.parse(data=input_file, format=input_format)
    rdf_graph = infer_rdf_graph(rdf_graph, infer)
    return rdf_graph

def load_rdf_dataset(dataset):
    data_path = DATA_DIR + dataset + ".pkl"
    if not os.path.isfile(data_path):
        raise ValueError("Please create the RDF dataset ({dataset}) before loading it. Run `geist create --help` for detailed information".format(dataset=dataset))
    with open(data_path, mode='rb') as f:
        geist_graph_object = pickle.load(f)
    rdf_graph, infer = geist_graph_object["rdf_graph"], geist_graph_object["infer"]
    return rdf_graph, infer

def delete_rdf_dataset(**kwargs):
    dataset = kwargs["dataset"] if "dataset" in kwargs else "kb"
    data_path = DATA_DIR + dataset + ".pkl"
    if not os.path.isfile(data_path):
        raise ValueError("Nothing to be removed. Can NOT find {data_path}".format(data_path=data_path))
    os.remove(data_path)
    return

def ensure_dir_exists(file_path):
    dir_path = os.path.split(file_path)[0]
    if (dir_path != '') and (not os.path.isdir(dir_path)):
        os.makedirs(dir_path)
    return

def query2df(rdf_graph, query):
    """
    This function is to run query on a RDF graph
    :param rdf_graph: a RDF Graph object supported by RDFLib
    :param query: string. A query to be applied to the given RDF graph
    :return res: a Pandas data frame. Results of the query
    """
    file = StringIO()
    JSONResultSerializer(rdf_graph.query(query)).serialize(file)
    res_json = json.loads(file.getvalue())
    bindings = res_json["results"]["bindings"]
    colnames = res_json["head"]["vars"]
    if bindings:
        # type: uri, literal, or bnode
        res_df = pd.DataFrame(bindings).apply(lambda row: row.apply(lambda x: "<"+x["value"]+">" if x["type"] == "uri" else '"'+x["value"]+'"' if x["type"] == "literal" else "_:"+x["value"]), axis=1)
    else:
        res_df = pd.DataFrame(columns=colnames)
    return res_df[colnames]

def visualize_query_results(query_res, edges, same_color=False):
    """
    This function is to visualize query results
    :param query_res: a Pandas data frame.
    :param edges: a list of list. [[start_node1, end_node1, label1], [start_node2, end_node2, label2], ...] where these items are column names
    :param same_color: a bool value to denote if all edges are filled with the same color (default: False)
    :return g: a Graphviz graph
    """
    # Create a directed graph
    G = pgv.AGraph(directed=True)
    # Add nodes and edges
    for _, row in query_res.iterrows():
        for idx, edge in enumerate(edges):
            if same_color:
                color = PASTEL_COLORS[0]
            else:
                color = PASTEL_COLORS[idx % 8]
            G.add_node(row[edge[0]], shape="box", style="filled, rounded", fillcolor=color)
            G.add_node(row[edge[1]], shape="box", style="filled, rounded", fillcolor=color)
            G.add_edge(row[edge[0]], row[edge[1]], label=row[edge[2]])
    return G

def _create(dataset, inputfile, inputformat, colnames, infer):
    """Create a new RDF dataset"""
    data_path = DATA_DIR + dataset + ".pkl"
    if os.path.isfile(data_path):
        raise ValueError("Please remove the existing RDF dataset ({dataset}) before loading the new one. Run `geist destroy --help` for detailed information".format(dataset=dataset))
    if inputformat == "csv" and (not colnames):
        raise ValueError("Please provide the column names of triples for the CSV file, i.e., --colnames")
    rdf_graph = create_rdf_graph(inputfile, inputformat, colnames, infer)
    # Save as a Gesit graph object
    geist_graph_object = {"rdf_graph": rdf_graph, "infer": infer}
    ensure_dir_exists(data_path)
    with open(data_path, "wb") as f:
        pickle.dump(geist_graph_object, f)
    return

def _load(dataset, inputfile, inputformat, colnames):
    """Import data into a RDF dataset"""
    if inputformat == "csv" and (not colnames):
        raise ValueError("Please provide the column names of triples for the CSV file, i.e., --colnames")
    (rdf_graph, infer) = load_rdf_dataset(dataset)
    rdf_graph = rdf_graph + create_rdf_graph(inputfile, inputformat, colnames, infer)
    geist_graph_object = {"rdf_graph": rdf_graph, "infer": infer}
    with open(DATA_DIR + dataset + ".pkl", "wb") as f:
        pickle.dump(geist_graph_object, f)
    return

def get_content(data, isfilepath):
    """
    This function is to retrieve the content of the given file path or the input string itself.
    :param data: a string. It can be a file path or content of the file
    :param isfilepath: a bool value to denote if the given data is a file path or not (default: True, which is a file path)
    :return content: a string
    """
    if isfilepath:
        with open(data.strip(), mode='r', encoding='utf-8') as fin:
            content = fin.read()
    else:
        content = data.strip()
    return content

class CreateExtension(ContainerTag):
    tags = {"create"}
    
    def render(self, dataset="kb", inputformat="json-ld", colnames=None, infer="none", isfilepath=True, caller=None):
        _create(dataset, get_content(str(caller()), isfilepath), inputformat, colnames, infer)
        return ""

class LoadExtension(ContainerTag):
    tags = {"load"}
    
    def render(self, dataset="kb", inputformat="json-ld", colnames=None, isfilepath=True, caller=None):
        _load(dataset, get_content(str(caller()), isfilepath), inputformat, colnames)
        return ""

class QueryExtension(ContainerTag):
    tags = {"query"}

    def render(self, dataset="kb", isfilepath=True, caller=None):
        (rdf_graph, _) = load_rdf_dataset(dataset)
        res = query2df(rdf_graph, get_content(str(caller()), isfilepath))
        return res.to_json()

class DestroyExtension(StandaloneTag):
    tags = {"destroy"}
    
    def render(self, dataset="kb"):
        delete_rdf_dataset(dataset=dataset)
        return ""

class GeistIncludeExtension(StandaloneTag):
    tags = {"geistinclude"}

    def render(self, filepath):
        return ""

def include_filepaths(data):
    """
    This function is to retrive file paths of the included files through matching the pattern {% geistinclude filepath %}
    :param data: a string, which might contains multiple included files
    :return file_paths: a list of file paths
    """
    blocks = re.findall(r"{%\s*geistinclude.*?\s*%}", data)
    file_paths = []
    for block in blocks:
        file_path = re.search(r"{%\s*geistinclude\s+([^\s]*?)\s*%}", block).groups()[0].replace('"', "").replace("'", "")
        file_paths.append(file_path)
    return file_paths

def generate_macros_class(file_paths):
    """
    This function is to generate a class named GeistMacrosExtension based on the give file paths
    :param file_paths: a list of file paths
    :return macros_class: a string of Python code to define the GeistMacrosExtension class
    """
    macros_code = ""
    for file_path in file_paths:
        with open(file_path, mode='r', encoding='utf-8') as fin:
            macros = fin.read()
        blocks = re.findall(r"{%\s*macro.*?{%\s*endmacro\s*%}", macros, re.DOTALL)
        for block in blocks:
            (tag, params, content) = re.search(r"{%\s*macro\s*([a-zA-Z\d_]+)\s*([a-zA-Z\d_ ]*).*?%}(.*?){%\s*endmacro\s*%}", block, re.DOTALL).groups()
            params_code = ", ".join(['{param}=kwargs["{param}"]'.format(param=param) for param in params.strip().split()])
            macros_code = macros_code + '''
            if tag == "{tag}":
                curr_env = jinja2.Environment()
                rendered_content = curr_env.from_string("""{content}""").render({params_code})
                return rendered_content'''.format(tag=tag, content=content, params_code=params_code)
    return """
class GeistMacrosExtension(StandaloneTag):
    tags = {"geistmacro"}

    def render(self, tag, *args, **kwargs):""" + macros_code

@cli.command()
@click.option('--dataset', '-d', default='kb', type=str, help='Name of RDF dataset to create (default "kb")')
@click.option('--inputfile', '-ifile', required=True, type=click.File('r'), default=sys.stdin, help='Path of the file to be loaded as triples')
@click.option('--inputformat', '-iformat', default='json-ld', type=click.Choice(['xml', 'n3', 'turtle', 'nt', 'pretty-xml', 'trix', 'trig', 'nquads', 'json-ld', 'hext', 'csv']), help='Format of the file to be loaded as triples (default json-ld)')
@click.option('--colnames', default=None, type=str, help='Column names of triples with the format of [[subject1, predicate1, object1], [subject2, predicate2, object2], ...] when the input format is csv')
@click.option('--infer', default='none', type=click.Choice(['none', 'rdfs', 'owl', 'rdfs_owl']), help='Inference to perform on update [none, rdfs, owl, rdfs_owl] (default "none")')
def create(dataset, inputfile, inputformat, colnames, infer):
    """Create a new RDF dataset"""
    _create(dataset, inputfile.read(), inputformat, colnames, infer)

@cli.command()
@click.option('--dataset', '-d', default='kb', type=str, help='Name of RDF dataset to create (default "kb")')
@click.option('--inputfile', '-ifile', required=True, type=click.File('r'), default=sys.stdin, help='Path of the file to be loaded as triples')
@click.option('--inputformat', '-iformat', default='json-ld', type=click.Choice(['xml', 'n3', 'turtle', 'nt', 'pretty-xml', 'trix', 'trig', 'nquads', 'json-ld', 'hext', 'csv']), help='Format of the file to be loaded as triples (default json-ld)')
@click.option('--colnames', default=None, type=str, help='Column names of triples with the format of [[subject1, predicate1, object1], [subject2, predicate2, object2], ...] when the input format is csv')
def load(dataset, inputfile, inputformat, colnames):
    """Import data into a RDF dataset"""
    _load(dataset, inputfile.read(), inputformat, colnames)

@cli.command()
@click.option('--dataset', '-d', default='kb', type=str, help='Name of RDF dataset to be removed (default "kb")')
def destroy(dataset):
    """Delete an RDF dataset"""
    delete_rdf_dataset(dataset=dataset)

@cli.command()
@click.option('--dataset', '-d', default='kb', type=str, help='Name of RDF dataset to be exported (default "kb")')
@click.option('--outputfile', '-ofile', default=None, type=str, help='Path of the file to store these exported triples (default: None)')
@click.option('--outputformat', '-oformat', default='nt', type=click.Choice(['json-ld', 'n3', 'nquads', 'nt', 'hext', 'pretty-xml', 'trig', 'trix', 'turtle', 'longturtle', 'xml']), help='Format of the exported triples (default nt)')
def export(dataset, outputfile, outputformat):
    """Export an RDF graph"""
    (rdf_graph, infer) = load_rdf_dataset(dataset)
    if outputfile is None:
        print(rdf_graph.serialize(format=outputformat))
    else:
        ensure_dir_exists(outputfile)
        rdf_graph.serialize(destination=outputfile, format=outputformat)

@cli.command()
@click.option('--dataset', '-d', default='kb', type=str, help='Name of RDF dataset to be queried (default "kb")')
@click.option('--file', required=True, type=click.File('r'), default=sys.stdin, help='Path of the file containing the SPARQL query to execute')
@click.option('--outputfile', '-ofile', default=None, type=str, help='Path of the file to store the query results (default: None)')
def query(dataset, file, outputfile):
    """Perform a SPARQL query on a dataset"""
    (rdf_graph, infer) = load_rdf_dataset(dataset)
    res = query2df(rdf_graph, file)
    if outputfile is None:
        print(res.to_markdown())
    else:
        ensure_dir_exists(outputfile)
        res.to_csv(outputfile, index=False)

@cli.command()
@click.option('--dataset', '-d', default='kb', type=str, help='Name of RDF dataset to be visualized (default "kb")')
@click.option('--mappings', '-m', default=None, help='File of the mappings to shorten text (str): path of a JSON file, where the key is the original text and the value is the shorter text.')
@click.option('--outputfile', '-ofile', default='res', type=str, help='Path of the file without extension to store the graph (default: res)')
@click.option('outputformats', '--outputformat', '-oformat', default=['none'], type=click.Choice(['none', 'png', 'gv']), multiple=True, help='Format of the graph (default: none): none or png or gv')
def graph(dataset, mappings, outputfile, outputformats):
    """Visualize a dataset"""
    if mappings:
        with open(mappings, mode='r', encoding='utf-8') as fin:
            mappings = json.loads(fin.read())
    (rdf_graph, infer) = load_rdf_dataset(dataset)
    query = """
            SELECT ?s ?p ?o
            WHERE {
                ?s ?p ?o
            }
        """
    res = query2df(rdf_graph, query).replace(mappings, regex=True)
    G = visualize_query_results(query_res=res, edges=[['s', 'o', 'p']], same_color=True)

    # Save the graph
    ensure_dir_exists(outputfile)
    for outputformat in set(outputformats):
        if outputformat == 'none':
            print(G.string())
        else:
            output_path = outputfile + '.' + outputformat
            if outputformat == 'png':
                G.layout(prog='dot')
                G.draw(output_path)
            else: # gv
                G.write(output_path)

@cli.command()
@click.option('--file', required=True, type=click.File('r'), default=sys.stdin, help='Path of the file containing the report template to expand')
@click.option('--outputfile', '-ofile', default='report', type=str, help='Path of the file without extension to store the report (default: report)')
@click.option('outputformats', '--outputformat', '-oformat', default=['none'], type=click.Choice(['none', 'png', 'gv', 'txt']), multiple=True, help='Format of the report (default: none): none or png or gv or txt, where none will print the report directly.')
def report(file, outputfile, outputformats):
    """Expand a report using a dataset"""

    # Define the GeistMacrosExtension class
    extensions = [CreateExtension, LoadExtension, QueryExtension, DestroyExtension, GeistIncludeExtension]
    content = file.read()
    file_paths = include_filepaths(content)
    if file_paths:
        macros_class = generate_macros_class(file_paths)
        exec(macros_class, globals())
        extensions.append(GeistMacrosExtension)

    # Render the report
    environment = jinja2.Environment(loader=jinja2.FileSystemLoader("./"), trim_blocks=True, extensions=extensions)
    environment.filters['json2df'] = json2df
    template = environment.from_string(content)
    report = template.render()

    # Save the report
    ensure_dir_exists(outputfile)
    for outputformat in set(outputformats):
        if outputformat == 'none':
            print(report)
        else:
            output_path = outputfile + '.' + outputformat
            if outputformat == 'png':
                graph = pgv.AGraph(string=report)
                graph.draw(output_path, format='png', prog='dot')
            else: # gv or txt
                with open(output_path, 'w') as fout:
                    fout.write(report)

if __name__ == '__main__':
    cli()
