from io import StringIO
import pandas as pd
import warnings, click, pickle, json, ast, os
from geist.tools.filters import csv2df, json2df
from geist.tools.utils import ensure_dir_exists, update_outputroot
from clingo.control import Control

DATA_DIR = ".geistdata/clingo/"
fact_data = []

def dict2facts(dict):
    facts = ""
    for predicate, arguments in dict.items():
        for argument in arguments:
            facts = facts + predicate + "(" + ",".join(argument) + ").\n"
    return facts

def dict2dfs(dict):
    dfs = {}
    for predicate, arguments in dict.items():
        dfs[predicate] = pd.DataFrame(columns=[f"arg{k+1}" for k in range(len(arguments[0]))], data=arguments) if arguments else pd.DataFrame()
    return dfs

def format_dicts(dicts, returnformat):
    if returnformat == 'dict':
        res = dicts
    else:
        res = []
        for one_case in dicts:
            if returnformat == 'lp':
                res.append(dict2facts(one_case))
            elif returnformat == 'df':
                res.append(dict2dfs(one_case))
            else:
                raise ValueError("Only 'dict', 'lp', and 'df' are supported for returnformat")
    return res

def filter_by_predicate(dicts, predicate):
    filtered_dicts = []
    for one_case in dicts:
        filtered_dict = {predicate: one_case[predicate]} if predicate in one_case else {predicate: []}
        filtered_dicts.append(filtered_dict)
    return filtered_dicts

def query2dicts(ctl, query=None, name="base", predicate=None):
    global fact_data
    fact_data = []
    if query:
        ctl.add(name, [], query)
        ctl.ground(parts=[(name, [])])
    ctl.configuration.solve.models = 0 # enable multiple solutions
    ctl.solve(on_model=collect_fact_data)
    # Filter by a given predicate
    if predicate:
        fact_data = filter_by_predicate(fact_data, predicate)
    return fact_data

def build_program(inputfile, inputformat, predicate="isfirstcol"):
    if inputformat == "lp":
        program = inputfile
    elif inputformat == "json":
        if type(inputfile)  == str:
            # This is the format generated by geist query(export) clingo
            inputfile = ast.literal_eval(inputfile)
            if len(inputfile) > 1:
                warnings.warn("There are multiple possibilities. Only the first one is considered.")
            inputfile = inputfile[0]
        program = dict2facts(inputfile)
    elif inputformat == "csv":
        df = csv2df(inputfile)
        if predicate == "isfirstcol":
            predicate, arguments = df.columns[0], df.columns[1:]
            program = "\n".join(df[predicate] + "(" + df[arguments].apply(lambda row: ", ".join(row.astype(str)), axis=1) + ").")
        else: # predicate itself is the predicate
            program = "\n".join(predicate + "(" + df.apply(lambda row: ", ".join(row.astype(str)), axis=1) + ").")        
    else:
        raise ValueError("Invalid input format. Only lp, csv, json are supported for now.")
    return program

def add_and_ground_program(program, name="base", ctl=None):
    if not ctl:
        ctl = Control()
    ctl.add(name, [], program)
    ctl.ground(parts=[(name, [])])
    return ctl

def create_asp_dataset(inputfile, inputformat, predicate="isfirstcol", name="base"):
    program = build_program(inputfile, inputformat, predicate)
    conn = add_and_ground_program(program, name, None)
    return conn, program

def program2memory(program, dataset):
    if dataset.startswith(':memory:'):
        global geist_clingo_facts
        if dataset == ':memory:':
            geist_clingo_facts = program
        else:
            try:
                geist_clingo_facts = {} if type(geist_clingo_facts) != dict else geist_clingo_facts
            except:
                geist_clingo_facts = {}
            geist_clingo_facts[dataset.replace(':memory:', '')] = program
    return

def collect_fact_data(model):
    one_case = {}
    for atom in model.symbols(shown=True):
        predicate = atom.name
        one_case.setdefault(predicate, []).append([str(argument) for argument in atom.arguments])
    fact_data.append(one_case)

def collect_and_save_fact_data(conn, dataset, is_create=True, save_to_file=True):
    global fact_data
    fact_data = []
    conn.solve(on_model=collect_fact_data)
    if len(fact_data) > 1:
        warnings.warn("There are multiple possibilities. Only the first one is saved to an ASP dataset.")
    if save_to_file:
        data_path = DATA_DIR + dataset + '.pkl'
        if is_create and os.path.isfile(data_path):
            raise ValueError("Please remove the existing ASP dataset ({dataset}) before loading the new one. Run `geist destroy --help` for detailed information".format(dataset=dataset))
        ensure_dir_exists(data_path, output=False)
        with open(data_path, "wb") as f:
            pickle.dump(fact_data[0], f)
    return dict2facts(fact_data[0])

def load_asp_dataset(dataset, name="base"):
    if isinstance(dataset, str):
        if dataset == ':memory:':
            conn = add_and_ground_program(globals()["geist_clingo_facts"], name, None)
        elif dataset.startswith(':memory:'):
            conn = add_and_ground_program(globals()["geist_clingo_facts"][dataset.replace(':memory:', '')], name, None)
        else:
            data_path = DATA_DIR + dataset + ".pkl"
            if not os.path.isfile(data_path):
                raise ValueError("Please create the ASP dataset ({dataset}) before loading it. Run `geist create clingo --help` for detailed information".format(dataset=dataset))
            with open(data_path, mode='rb') as f:
                facts = dict2facts(pickle.load(f))
            conn = add_and_ground_program(facts, name, None)
    else: # For Python API Control class
        conn = dataset
    return conn

@click.group()
def cli():
    pass

@cli.group()
def clingo():
    pass

def clingo_create(dataset, inputfile, inputformat, predicate, programname):
    """Create a new ASP dataset using Clingo"""
    conn, program = create_asp_dataset(inputfile, inputformat, predicate, programname)
    program2memory(program, dataset)
    if not dataset.startswith(':memory:'):
        collect_and_save_fact_data(conn, dataset, True, True)
    return conn

def clingo_load(dataset, inputfile, inputformat, predicate, programname, inmemory):
    """Import data into an ASP dataset"""
    conn = load_asp_dataset(dataset)
    program = build_program(inputfile, inputformat, predicate)
    conn = add_and_ground_program(program, name=programname, ctl=conn)
    save_to_file = not inmemory and not dataset.startswith(':memory:')
    program = collect_and_save_fact_data(conn, dataset, False, save_to_file)
    if not save_to_file:
        program2memory(program, dataset)
    return conn

def clingo_query(dataset, inputfile, hasoutput, outputroot, outputfile, returnformat='lp', predicate=None, programname='base'):
    """Perform an ASP query on a dataset"""
    update_outputroot(outputroot)
    conn = load_asp_dataset(dataset)
    dicts = query2dicts(conn, inputfile, programname, predicate)

    if hasoutput and outputfile:
        outputfile = ensure_dir_exists(outputfile)
        with open(outputfile, 'w') as fout:
            json.dump(dicts, fout)
    res = format_dicts(dicts, returnformat)

    if hasoutput and not outputfile:
        print("\n".join(f"---Option {k}---\n{item}" for k, item in enumerate(res, 1)))
    return res, conn

def clingo_destroy(**kwargs):
    """Delete an ASP dataset"""
    dataset = kwargs["dataset"] if "dataset" in kwargs else "kb"
    data_path = DATA_DIR + dataset + ".pkl"
    if not os.path.isfile(data_path):
        if "quiet" in kwargs and kwargs["quiet"]:
            return
        raise ValueError("Nothing to be removed. Can NOT find {data_path}".format(data_path=data_path))
    os.remove(data_path)
    return

def clingo_export(dataset, predicate, hasoutput, outputroot, outputfile, returnformat='lp', programname='base'):
    """Export facts"""
    return clingo_query(dataset, None, hasoutput, outputroot, outputfile, returnformat, predicate, programname)



if __name__ == '__main__':
    cli()
